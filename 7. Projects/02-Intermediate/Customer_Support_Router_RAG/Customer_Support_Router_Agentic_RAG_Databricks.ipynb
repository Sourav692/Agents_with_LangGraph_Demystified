{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build a Customer Support Router Agentic RAG System (Databricks Vector Search)\n",
        "\n",
        "In this project, we will leverage the power of AI Agents and RAG Systems to build an intelligent Router Agentic RAG System to handle customer support queries using a custom knowledgebase powered by **Databricks Vector Search**.\n",
        "\n",
        "![](https://i.imgur.com/bLCdxCI.png)\n",
        "\n",
        "### Intelligent Router Agentic RAG System\n",
        "\n",
        "This project focuses on building an **Intelligent Router Agentic RAG System** that combines intelligent query analysis, sentiment detection, and dynamic routing with Retrieval-Augmented Generation (RAG) to handle diverse user inquiries efficiently. The workflow includes the following components:\n",
        "\n",
        "1. **Query Categorization and Sentiment Analysis**:\n",
        "   - The system uses an LLM to analyze the user's query and determine:\n",
        "     - **Query Category**: Identifies the type of problem, such as billing, technical issues, or general queries.\n",
        "     - **User Sentiment**: Evaluates the user's sentiment (positive, neutral, or negative) to determine if escalation is needed.\n",
        "\n",
        "2. **Intelligent Routing**:\n",
        "   - Based on the **query_category** and **query_sentiment**, the system routes the query to the appropriate handling node:\n",
        "     - **Escalate to Human**: If the sentiment is negative, the query is escalated to a human for resolution.\n",
        "     - **Generate Billing Response**: Queries related to billing are routed to generate an appropriate response.\n",
        "     - **Generate Technical Response**: Technical queries are routed for a specialized technical response.\n",
        "     - **Generate General Response**: General queries are handled with context-aware responses.\n",
        "\n",
        "3. **Knowledge Base Integration (RAG) - Databricks Vector Search**:\n",
        "   - The system integrates with a **Databricks Vector Search Index** to augment responses with relevant and accurate information.\n",
        "   - Retrieval-Augmented Generation (RAG) ensures that responses are grounded in the latest and most reliable data.\n",
        "   - Uses **Delta Sync Index** with managed embeddings (`databricks-gte-large-en`).\n",
        "\n",
        "4. **Escalation Mechanism**:\n",
        "   - Negative sentiment triggers an **escalation to a human**, ensuring the user receives empathetic and personalized support for critical issues.\n",
        "\n",
        "### What This Notebook Covers\n",
        "\n",
        "This notebook is **self-contained** and walks through:\n",
        "1. **Loading** the knowledge base from JSON.\n",
        "2. **Creating a Delta table** with the document chunks.\n",
        "3. **Setting up a Databricks Vector Search Endpoint** and **Index** with managed embeddings (`databricks-gte-large-en`).\n",
        "4. **Building the LangGraph Router Agent** with RAG powered by Databricks Vector Search.\n",
        "5. **Testing** the full workflow with sample customer queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langchain==0.3.14\n",
        "!pip install -q langchain-openai==0.3.0\n",
        "!pip install -q langchain-community==0.3.14\n",
        "!pip install -q langgraph==0.2.64\n",
        "!pip install -q databricks-vectorsearch\n",
        "!pip install -U -qqqq databricks-langchain\n",
        "!pip install -q jq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load the Company Knowledge Base\n",
        "\n",
        "Load the customer support knowledge base from the JSON file. Each document contains a `text` field and `metadata` with a `category` (technical, billing, or general)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"./docs/router_agent_documents.json\", \"r\") as f:\n",
        "    knowledge_base = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(knowledge_base)} documents from knowledge base.\")\n",
        "knowledge_base[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create a Delta Table with Document Chunks\n",
        "\n",
        "We convert the knowledge base documents into a Spark DataFrame and save it as a **Delta table** in Unity Catalog. This Delta table will serve as the source for the Vector Search index.\n",
        "\n",
        "| Component | Value |\n",
        "|---|---|\n",
        "| **Catalog** | `agentic_ai` |\n",
        "| **Schema** | `langgraph` |\n",
        "| **Table** | `router_agent_chunks` |\n",
        "| **Columns** | `chunk_id` (INT), `content` (STRING), `category` (STRING) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unity Catalog configuration\n",
        "CATALOG = \"agentic_ai\"\n",
        "SCHEMA = \"langgraph\"\n",
        "TABLE_NAME = f\"{CATALOG}.{SCHEMA}.router_agent_chunks\"\n",
        "\n",
        "# Convert knowledge base to chunk data\n",
        "chunk_data = []\n",
        "for i, doc in enumerate(knowledge_base):\n",
        "    chunk_data.append({\n",
        "        \"chunk_id\": i + 1,\n",
        "        \"content\": doc[\"text\"],\n",
        "        \"category\": doc[\"metadata\"][\"category\"]\n",
        "    })\n",
        "\n",
        "print(f\"Created {len(chunk_data)} chunks.\")\n",
        "chunk_data[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Spark DataFrame and save as Delta table\n",
        "spark_df = spark.createDataFrame(chunk_data)\n",
        "\n",
        "spark_df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .saveAsTable(TABLE_NAME)\n",
        "\n",
        "print(f\"Delta table '{TABLE_NAME}' created successfully.\")\n",
        "display(spark.table(TABLE_NAME))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable Change Data Feed (required for Delta Sync Index)\n",
        "spark.sql(f\"ALTER TABLE {TABLE_NAME} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
        "print(\"Change Data Feed enabled on the Delta table.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create a Vector Search Endpoint\n",
        "\n",
        "A Vector Search **endpoint** is a compute resource that serves the vector search index. We create a `STANDARD` endpoint named `router_agent_endpoint`.\n",
        "\n",
        "> **Note**: If the endpoint already exists, this cell will raise an error — you can safely skip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from databricks.vector_search.client import VectorSearchClient\n",
        "\n",
        "ENDPOINT_NAME = \"router_agent_endpoint\"\n",
        "\n",
        "vs_client = VectorSearchClient()\n",
        "\n",
        "# Create the Vector Search endpoint (skip if it already exists)\n",
        "try:\n",
        "    vs_client.create_endpoint(\n",
        "        name=ENDPOINT_NAME,\n",
        "        endpoint_type=\"STANDARD\"\n",
        "    )\n",
        "    print(f\"Vector Search endpoint '{ENDPOINT_NAME}' created. It may take a few minutes to provision.\")\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        print(f\"Endpoint '{ENDPOINT_NAME}' already exists. Skipping creation.\")\n",
        "    else:\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create the Vector Search Index (with Managed Embeddings)\n",
        "\n",
        "We create a **Delta Sync Index** on the Delta table. This index:\n",
        "- Uses **`databricks-gte-large-en`** as the managed embedding model — Databricks automatically computes embeddings from the `content` column.\n",
        "- Syncs automatically with the source Delta table via `TRIGGERED` pipeline.\n",
        "- Uses `chunk_id` as the primary key.\n",
        "\n",
        "> **Note**: If the index already exists, this cell will raise an error — you can safely skip it. The index creation may take several minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "INDEX_NAME = f\"{CATALOG}.{SCHEMA}.router_agent_index\"\n",
        "\n",
        "# Create the Delta Sync Index with managed embeddings\n",
        "try:\n",
        "    index = vs_client.create_delta_sync_index(\n",
        "        endpoint_name=ENDPOINT_NAME,\n",
        "        source_table_name=TABLE_NAME,\n",
        "        index_name=INDEX_NAME,\n",
        "        pipeline_type=\"TRIGGERED\",\n",
        "        primary_key=\"chunk_id\",\n",
        "        embedding_source_column=\"content\",\n",
        "        embedding_model_endpoint_name=\"databricks-gte-large-en\"\n",
        "    )\n",
        "    print(f\"Vector Search index '{INDEX_NAME}' created successfully.\")\n",
        "    print(\"Note: Embedding computation may take a few minutes. Wait for the index to become ONLINE before querying.\")\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        print(f\"Index '{INDEX_NAME}' already exists. Skipping creation.\")\n",
        "    else:\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Wait for the index to become ONLINE\n",
        "vs_index = vs_client.get_index(endpoint_name=ENDPOINT_NAME, index_name=INDEX_NAME)\n",
        "\n",
        "status = vs_index.describe()\n",
        "print(f\"Index status: {status.get('status', {})}\")\n",
        "\n",
        "# Poll until the index is ready (optional - you can also check manually in the Databricks UI)\n",
        "while status.get(\"status\", {}).get(\"ready\") != True:\n",
        "    print(\"Index is not ready yet. Waiting 30 seconds...\")\n",
        "    time.sleep(30)\n",
        "    status = vs_index.describe()\n",
        "    print(f\"Index status: {status.get('status', {})}\")\n",
        "\n",
        "print(\"Index is ONLINE and ready for queries!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Environment Variables & LLM\n",
        "\n",
        "We use **Databricks Foundation Model APIs** via `ChatDatabricks` so no external API keys are needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from databricks_langchain import ChatDatabricks\n",
        "\n",
        "# Use a Databricks-hosted LLM endpoint\n",
        "# Options: \"databricks-claude-3-7-sonnet\", \"databricks-gpt-oss-120b\", \"databricks-meta-llama-3-3-70b-instruct\"\n",
        "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
        "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Connect to the Vector Search Index\n",
        "\n",
        "Now we connect to the Vector Search index we just created (or the pre-existing one) to use it for retrieval in our RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the vector search index (uses ENDPOINT_NAME and INDEX_NAME defined earlier)\n",
        "vs_index = vs_client.get_index(endpoint_name=ENDPOINT_NAME, index_name=INDEX_NAME)\n",
        "print(f\"Connected to Vector Search Index: {INDEX_NAME}\")\n",
        "print(f\"Endpoint: {ENDPOINT_NAME}\")\n",
        "print(f\"Source Table: {TABLE_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper: Convert Vector Search Results to LangChain Documents\n",
        "\n",
        "Databricks Vector Search returns results in its own format. This helper converts them to LangChain `Document` objects for seamless integration with our RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "from typing import List\n",
        "\n",
        "def convert_vector_search_to_documents(results) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Convert Databricks Vector Search results to LangChain Document objects.\n",
        "    \n",
        "    The first column retrieved is loaded into page_content,\n",
        "    and the rest (except the score column) into metadata.\n",
        "    \"\"\"\n",
        "    column_names = [col[\"name\"] for col in results[\"manifest\"][\"columns\"]]\n",
        "    \n",
        "    langchain_docs = []\n",
        "    for item in results[\"result\"][\"data_array\"]:\n",
        "        metadata = {}\n",
        "        # Last element is the similarity score\n",
        "        score = item[-1]\n",
        "        # First element is page_content, middle elements are metadata\n",
        "        for i in range(1, len(item) - 1):\n",
        "            metadata[column_names[i]] = item[i]\n",
        "        metadata[\"score\"] = score\n",
        "        doc = Document(page_content=item[0], metadata=metadata)\n",
        "        langchain_docs.append(doc)\n",
        "    \n",
        "    return langchain_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Vector Search Retrieval\n",
        "\n",
        "Let's verify that the vector search index is working correctly with some sample queries and metadata filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Query with 'general' category filter\n",
        "query = 'what is your refund policy?'\n",
        "results = vs_index.similarity_search(\n",
        "    query_text=query,\n",
        "    columns=[\"content\", \"category\"],\n",
        "    num_results=3,\n",
        "    filters={\"category\": [\"general\"]},\n",
        "    query_type=\"hybrid\"\n",
        ")\n",
        "\n",
        "docs = convert_vector_search_to_documents(results)\n",
        "for doc in docs:\n",
        "    print(f\"Score: {doc.metadata['score']:.4f} | Category: {doc.metadata['category']}\")\n",
        "    print(f\"Content: {doc.page_content[:150]}...\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Query with 'technical' category filter\n",
        "query = 'do you support pre-trained models?'\n",
        "results = vs_index.similarity_search(\n",
        "    query_text=query,\n",
        "    columns=[\"content\", \"category\"],\n",
        "    num_results=3,\n",
        "    filters={\"category\": [\"technical\"]},\n",
        "    query_type=\"hybrid\"\n",
        ")\n",
        "\n",
        "docs = convert_vector_search_to_documents(results)\n",
        "for doc in docs:\n",
        "    print(f\"Score: {doc.metadata['score']:.4f} | Category: {doc.metadata['category']}\")\n",
        "    print(f\"Content: {doc.page_content[:150]}...\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Query with 'billing' category filter\n",
        "query = 'what payment methods do you accept?'\n",
        "results = vs_index.similarity_search(\n",
        "    query_text=query,\n",
        "    columns=[\"content\", \"category\"],\n",
        "    num_results=3,\n",
        "    filters={\"category\": [\"billing\"]},\n",
        "    query_type=\"hybrid\"\n",
        ")\n",
        "\n",
        "docs = convert_vector_search_to_documents(results)\n",
        "for doc in docs:\n",
        "    print(f\"Score: {doc.metadata['score']:.4f} | Category: {doc.metadata['category']}\")\n",
        "    print(f\"Content: {doc.page_content[:150]}...\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Customer Inquiry State\n",
        "\n",
        "We create a `CustomerSupportState` typed dictionary to keep track of each interaction:\n",
        "- **customer_query**: The text of the customer's question\n",
        "- **query_category**: Technical, Billing, or General (used for routing)\n",
        "- **query_sentiment**: Positive, Neutral, or Negative (used for routing)\n",
        "- **final_response**: The system's response to the customer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Literal\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CustomerSupportState(TypedDict):\n",
        "    \"\"\"\n",
        "    customer_query: the original query from the customer.\n",
        "    query_category: the topic of the query (e.g., Technical, Billing).\n",
        "    query_sentiment: the emotional tone (e.g., Positive, Negative).\n",
        "    final_response: the system-generated response.\n",
        "    \"\"\"\n",
        "    customer_query: str\n",
        "    query_category: str\n",
        "    query_sentiment: str\n",
        "    final_response: str\n",
        "\n",
        "class QueryCategory(BaseModel):\n",
        "    categorized_topic: Literal['Technical', 'Billing', 'General']\n",
        "\n",
        "class QuerySentiment(BaseModel):\n",
        "    sentiment: Literal['Positive', 'Neutral', 'Negative']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explain TypeDict, Literal and BaseModel\n",
        "* **TypedDict**: Allows you to define a dictionary with a specific schema (i.e., typed keys and values). This is a lightweight structure used for state management in memory (not validated like Pydantic models).\n",
        "* **Literal**: Restricts a value to a fixed set of string options.\n",
        "* **BaseModel** from pydantic: Used for data validation and parsing using Python type hints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "QueryCategory(categorized_topic='Billing')  # ✅ Valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# QueryCategory(categorized_topic='billing')    # ❌ Validation error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Node Functions\n",
        "\n",
        "Each function below represents a stage in processing a customer inquiry:\n",
        "\n",
        "1. **categorize_inquiry**: Classifies the query into Technical, Billing, or General.\n",
        "2. **analyze_inquiry_sentiment**: Determines if the sentiment is Positive, Neutral, or Negative.\n",
        "3. **generate_technical_response**: Produces a response for technical issues using **Databricks Vector Search**.\n",
        "4. **generate_billing_response**: Produces a response for billing questions using **Databricks Vector Search**.\n",
        "5. **generate_general_response**: Produces a response for general queries using **Databricks Vector Search**.\n",
        "6. **escalate_to_human_agent**: Escalates the query to a human if sentiment is negative.\n",
        "7. **determine_route**: Routes the inquiry to the appropriate response node based on category and sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def categorize_inquiry(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "    \"\"\"\n",
        "    Classify the customer query into Technical, Billing, or General.\n",
        "    \"\"\"\n",
        "\n",
        "    query = support_state[\"customer_query\"]\n",
        "    ROUTE_CATEGORY_PROMPT = \"\"\"Act as a customer support agent trying to best categorize the customer query.\n",
        "                               You are an agent for an AI products and hardware company.\n",
        "\n",
        "                               Please read the customer query below and\n",
        "                               determine the best category from the following list:\n",
        "\n",
        "                               'Technical', 'Billing', or 'General'.\n",
        "\n",
        "                               Remember:\n",
        "                                - Technical queries will focus more on technical aspects like AI models, hardware, software related queries etc.\n",
        "                                - General queries will focus more on general aspects like contacting support, finding things, policies etc.\n",
        "                                - Billing queries will focus more on payment and purchase related aspects\n",
        "\n",
        "                                Return just the category name (from one of the above)\n",
        "\n",
        "                                Query:\n",
        "                                {customer_query}\n",
        "                            \"\"\"\n",
        "    prompt = ROUTE_CATEGORY_PROMPT.format(customer_query=query)\n",
        "    route_category = llm.with_structured_output(QueryCategory).invoke(prompt)\n",
        "\n",
        "    return {\n",
        "        \"query_category\": route_category.categorized_topic\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorize_inquiry({\"customer_query\": \"Do you provide pretrained models?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorize_inquiry({\"customer_query\": \"what is your refund policy?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorize_inquiry({\"customer_query\": \"what payment methods are accepted?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_inquiry_sentiment(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "    \"\"\"\n",
        "    Analyze the sentiment of the customer query as Positive, Neutral, or Negative.\n",
        "    \"\"\"\n",
        "\n",
        "    query = support_state[\"customer_query\"]\n",
        "    SENTIMENT_CATEGORY_PROMPT = \"\"\"Act as a customer support agent trying to best categorize the customer query's sentiment.\n",
        "                                   You are an agent for an AI products and hardware company.\n",
        "\n",
        "                                   Please read the customer query below,\n",
        "                                   analyze its sentiment which should be one from the following list:\n",
        "\n",
        "                                   'Positive', 'Neutral', or 'Negative'.\n",
        "\n",
        "                                   Return just the sentiment (from one of the above)\n",
        "\n",
        "                                   Query:\n",
        "                                   {customer_query}\n",
        "                                \"\"\"\n",
        "    prompt = SENTIMENT_CATEGORY_PROMPT.format(customer_query=query)\n",
        "    sentiment_category = llm.with_structured_output(QuerySentiment).invoke(prompt)\n",
        "\n",
        "    return {\n",
        "        \"query_sentiment\": sentiment_category.sentiment\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze_inquiry_sentiment({\"customer_query\": \"what is your refund policy?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze_inquiry_sentiment({\"customer_query\": \"what is your refund policy? I am really fed up with this product and need to refund it\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "def generate_technical_response(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "    \"\"\"\n",
        "    Provide a technical support response by combining knowledge from Databricks Vector Search and LLM.\n",
        "    \"\"\"\n",
        "    categorized_topic = support_state[\"query_category\"]\n",
        "    query = support_state[\"customer_query\"]\n",
        "\n",
        "    if categorized_topic.lower() == \"technical\":\n",
        "        # Perform retrieval from Databricks Vector Search with category filter\n",
        "        results = vs_index.similarity_search(\n",
        "            query_text=query,\n",
        "            columns=[\"content\", \"category\"],\n",
        "            num_results=3,\n",
        "            filters={\"category\": [\"technical\"]},\n",
        "            query_type=\"hybrid\"\n",
        "        )\n",
        "        relevant_docs = convert_vector_search_to_documents(results)\n",
        "        retrieved_content = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_template(\n",
        "            \"\"\"\n",
        "            Craft a clear and detailed technical support response for the following customer query.\n",
        "            Use the provided knowledge base information to enrich your response.\n",
        "            In case there is no knowledge base information or you do not know the answer just say:\n",
        "\n",
        "            Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\n",
        "\n",
        "            Customer Query:\n",
        "            {customer_query}\n",
        "\n",
        "            Relevant Knowledge Base Information:\n",
        "            {retrieved_content}\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = prompt | llm\n",
        "        tech_reply = chain.invoke({\n",
        "            \"customer_query\": query,\n",
        "            \"retrieved_content\": retrieved_content\n",
        "        }).content\n",
        "    else:\n",
        "        tech_reply = \"Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\"\n",
        "\n",
        "    return {\n",
        "        \"final_response\": tech_reply\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_technical_response({\"customer_query\": \"what is your refund policy?\", \"query_category\": \"General\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_technical_response({\"customer_query\": \"do you support on-prem models?\", \"query_category\": \"Technical\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_billing_response(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "    \"\"\"\n",
        "    Provide a billing support response by combining knowledge from Databricks Vector Search and LLM.\n",
        "    \"\"\"\n",
        "    categorized_topic = support_state[\"query_category\"]\n",
        "    query = support_state[\"customer_query\"]\n",
        "\n",
        "    if categorized_topic.lower() == \"billing\":\n",
        "        # Perform retrieval from Databricks Vector Search with category filter\n",
        "        results = vs_index.similarity_search(\n",
        "            query_text=query,\n",
        "            columns=[\"content\", \"category\"],\n",
        "            num_results=3,\n",
        "            filters={\"category\": [\"billing\"]},\n",
        "            query_type=\"hybrid\"\n",
        "        )\n",
        "        relevant_docs = convert_vector_search_to_documents(results)\n",
        "        retrieved_content = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_template(\n",
        "            \"\"\"\n",
        "            Craft a clear and detailed billing support response for the following customer query.\n",
        "            Use the provided knowledge base information to enrich your response.\n",
        "            In case there is no knowledge base information or you do not know the answer just say:\n",
        "\n",
        "            Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\n",
        "\n",
        "            Customer Query:\n",
        "            {customer_query}\n",
        "\n",
        "            Relevant Knowledge Base Information:\n",
        "            {retrieved_content}\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = prompt | llm\n",
        "        billing_reply = chain.invoke({\n",
        "            \"customer_query\": query,\n",
        "            \"retrieved_content\": retrieved_content\n",
        "        }).content\n",
        "    else:\n",
        "        billing_reply = \"Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\"\n",
        "\n",
        "    return {\n",
        "        \"final_response\": billing_reply\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_billing_response({\"customer_query\": \"what payment methods are supported?\", \"query_category\": \"Billing\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_general_response(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "    \"\"\"\n",
        "    Provide a general support response by combining knowledge from Databricks Vector Search and LLM.\n",
        "    \"\"\"\n",
        "    categorized_topic = support_state[\"query_category\"]\n",
        "    query = support_state[\"customer_query\"]\n",
        "\n",
        "    if categorized_topic.lower() == \"general\":\n",
        "        # Perform retrieval from Databricks Vector Search with category filter\n",
        "        results = vs_index.similarity_search(\n",
        "            query_text=query,\n",
        "            columns=[\"content\", \"category\"],\n",
        "            num_results=3,\n",
        "            filters={\"category\": [\"general\"]},\n",
        "            query_type=\"hybrid\"\n",
        "        )\n",
        "        relevant_docs = convert_vector_search_to_documents(results)\n",
        "        retrieved_content = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_template(\n",
        "            \"\"\"\n",
        "            Craft a clear and detailed general support response for the following customer query.\n",
        "            Use the provided knowledge base information to enrich your response.\n",
        "            In case there is no knowledge base information or you do not know the answer just say:\n",
        "\n",
        "            Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\n",
        "\n",
        "            Customer Query:\n",
        "            {customer_query}\n",
        "\n",
        "            Relevant Knowledge Base Information:\n",
        "            {retrieved_content}\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        chain = prompt | llm\n",
        "        general_reply = chain.invoke({\n",
        "            \"customer_query\": query,\n",
        "            \"retrieved_content\": retrieved_content\n",
        "        }).content\n",
        "    else:\n",
        "        general_reply = \"Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\"\n",
        "\n",
        "    return {\n",
        "        \"final_response\": general_reply\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_general_response({\"customer_query\": \"what is your refund policy?\", \"query_category\": \"General\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def escalate_to_human_agent(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "    \"\"\"\n",
        "    Escalate the query to a human agent if sentiment is negative.\n",
        "    \"\"\"\n",
        "\n",
        "    return {\n",
        "        \"final_response\": \"Apologies, we are really sorry! Someone from our team will be reaching out to your shortly!\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def determine_route(support_state: CustomerSupportState) -> str:\n",
        "    \"\"\"\n",
        "    Route the inquiry based on sentiment and category.\n",
        "    \"\"\"\n",
        "    if support_state[\"query_sentiment\"] == \"Negative\":\n",
        "        return \"escalate_to_human_agent\"\n",
        "    elif support_state[\"query_category\"] == \"Technical\":\n",
        "        return \"generate_technical_response\"\n",
        "    elif support_state[\"query_category\"] == \"Billing\":\n",
        "        return \"generate_billing_response\"\n",
        "    else:\n",
        "        return \"generate_general_response\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build and Compile the Workflow\n",
        "\n",
        "We construct a LangGraph workflow with the nodes defined above:\n",
        "1. **categorize_inquiry** → **analyze_inquiry_sentiment** → **route** to the proper response node.\n",
        "2. If negative, escalate to a human agent.\n",
        "3. Otherwise, produce an appropriate response (technical, billing, or general) using **Databricks Vector Search** for retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Create the graph with our typed state\n",
        "customer_support_graph = StateGraph(CustomerSupportState)\n",
        "\n",
        "# Add nodes for each function\n",
        "customer_support_graph.add_node(\"categorize_inquiry\", categorize_inquiry)\n",
        "customer_support_graph.add_node(\"analyze_inquiry_sentiment\", analyze_inquiry_sentiment)\n",
        "customer_support_graph.add_node(\"generate_technical_response\", generate_technical_response)\n",
        "customer_support_graph.add_node(\"generate_billing_response\", generate_billing_response)\n",
        "customer_support_graph.add_node(\"generate_general_response\", generate_general_response)\n",
        "customer_support_graph.add_node(\"escalate_to_human_agent\", escalate_to_human_agent)\n",
        "\n",
        "# Add edges to represent the processing flow\n",
        "customer_support_graph.add_edge(\"categorize_inquiry\", \"analyze_inquiry_sentiment\")\n",
        "customer_support_graph.add_conditional_edges(\n",
        "    \"analyze_inquiry_sentiment\",\n",
        "    determine_route,\n",
        "    [\n",
        "        \"generate_technical_response\",\n",
        "        \"generate_billing_response\",\n",
        "        \"generate_general_response\",\n",
        "        \"escalate_to_human_agent\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "# All terminal nodes lead to the END\n",
        "customer_support_graph.add_edge(\"generate_technical_response\", END)\n",
        "customer_support_graph.add_edge(\"generate_billing_response\", END)\n",
        "customer_support_graph.add_edge(\"generate_general_response\", END)\n",
        "customer_support_graph.add_edge(\"escalate_to_human_agent\", END)\n",
        "\n",
        "# Set the entry point for the workflow\n",
        "customer_support_graph.set_entry_point(\"categorize_inquiry\")\n",
        "\n",
        "# Compile the graph into a runnable agent\n",
        "memory = MemorySaver()\n",
        "compiled_support_agent = customer_support_graph.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the Workflow\n",
        "\n",
        "Below is a generated diagram of the workflow using Mermaid syntax. It shows how each node connects in the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, Image, Markdown\n",
        "\n",
        "display(Image(compiled_support_agent.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Function to Run the Workflow\n",
        "\n",
        "This function takes a customer query and runs it through our compiled workflow, returning the final results (category, sentiment, and generated response)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_support_agent(agent, prompt, user_session_id, verbose=False):\n",
        "    events = agent.stream(\n",
        "        {\"customer_query\": prompt},  # initial state of the agent\n",
        "        {\"configurable\": {\"thread_id\": user_session_id}},\n",
        "        stream_mode=\"values\",\n",
        "    )\n",
        "\n",
        "    print('Running Agent. Please wait...')\n",
        "    for event in events:\n",
        "        if verbose:\n",
        "                print(event)\n",
        "\n",
        "    display(Markdown(event['final_response']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the Customer Support Workflow\n",
        "\n",
        "Let's test the workflow with some sample queries to verify categorization, sentiment analysis, and response generation using **Databricks Vector Search** as the knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "uid = 'jim001'\n",
        "query = \"do you support pre-trained models?\"\n",
        "call_support_agent(agent=compiled_support_agent,\n",
        "                   prompt=query,\n",
        "                   user_session_id=uid,\n",
        "                   verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "uid = 'jim002'\n",
        "query = \"how do I get my invoice?\"\n",
        "call_support_agent(agent=compiled_support_agent,\n",
        "                   prompt=query,\n",
        "                   user_session_id=uid,\n",
        "                   verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"Can you tell me about your shipping policy?\"\n",
        "call_support_agent(agent=compiled_support_agent,\n",
        "                   prompt=query,\n",
        "                   user_session_id=uid,\n",
        "                   verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"I'm fed up with this faulty hardware, I need a refund\"\n",
        "call_support_agent(agent=compiled_support_agent,\n",
        "                   prompt=query,\n",
        "                   user_session_id=uid,\n",
        "                   verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What are your working hours?\"\n",
        "call_support_agent(agent=compiled_support_agent,\n",
        "                   prompt=query,\n",
        "                   user_session_id=uid,\n",
        "                   verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What have I asked you till now\"\n",
        "call_support_agent(agent=compiled_support_agent,\n",
        "                   prompt=query,\n",
        "                   user_session_id=uid,\n",
        "                   verbose=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
