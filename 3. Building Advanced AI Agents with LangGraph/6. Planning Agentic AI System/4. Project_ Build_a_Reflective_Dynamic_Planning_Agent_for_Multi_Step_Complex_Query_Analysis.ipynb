{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8400ced-9ed0-42f8-a614-8fd66e26033e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1kiH8lf1y4sD"
   },
   "source": [
    "# Build a Reflective Dynamic Planning Agent for Multi-Step Complex Query Analysis with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb900b5-8d68-4c5c-8f61-494d32f667fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NYBpZTjLnEXb"
   },
   "source": [
    "In this project we will be building a Planning Agent which can generate dynamic plans and iterate and update using reflection to help with multi-step complex query analysis.\n",
    "\n",
    "![](https://i.imgur.com/odnq3de.png)\n",
    "\n",
    "### Reflective Dynamic Planning Agent for Multi-Step Complex Query Analysis\n",
    "\n",
    "This project focuses on building a **Reflective Dynamic Planning Agent for Multi-Step Complex Query Analysis**, designed to break down user queries or tasks into manageable steps, execute them iteratively, and update the plan dynamically through reflection and replanning. The components of the system are as follows:\n",
    "\n",
    "1. **Plan Creation**:\n",
    "   - The agent begins by analyzing the user's input (topic or question) to create a **step-by-step plan** of sub-questions or steps needed to solve the overall task.\n",
    "   - Each step is clearly outlined in the **step-by-step plan** for sequential execution.\n",
    "\n",
    "2. **Step Execution**:\n",
    "   - The agent executes one step at a time using **OpenAI GPT-4o**.\n",
    "   - During execution:\n",
    "     - External tools, such as the **web search tool**, are used to gather required information or data for the step.\n",
    "     - The result of the executed step is captured and stored in the agent's state.\n",
    "\n",
    "3. **Reflection and Replanning**:\n",
    "   - After each step, the system evaluates the results and sends the **overall plan**, **steps executed**, **results**, and **remaining steps** to the **Replanner**.\n",
    "   - The **Replanner**:\n",
    "     - Assesses whether the remaining steps need modification based on the current results and any new insights.\n",
    "     - Updates the remaining steps if necessary, ensuring alignment with the overall objective.\n",
    "   - This iterative loop ensures adaptability to errors, incomplete data, or unforeseen circumstances.\n",
    "\n",
    "4. **Dynamic Feedback Loop**:\n",
    "   - The system monitors the status of the response:\n",
    "     - If all steps are executed successfully or an error occurs, the process halts.\n",
    "     - If there are remaining steps, the agent continues executing them one at a time, integrating results dynamically.\n",
    "\n",
    "5. **Agent State Maintenance**:\n",
    "   - Throughout the process, the agent maintains a record of:\n",
    "     - The **original plan**.\n",
    "     - **Executed steps and their results**.\n",
    "     - **Remaining steps** for further execution.\n",
    "   - This stateful approach ensures traceability and transparency in the agentâ€™s decision-making process.\n",
    "\n",
    "6. **Final Response Compilation**:\n",
    "   - Once all steps are executed or the process encounters an error, the agent compiles everything to generate the final reponse.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e93512bc-76cc-43d9-92b6-c63cc8ff28f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9hEI3WL328vZ"
   },
   "source": [
    "## Install OpenAI, LangGraph and LangChain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d22257-c070-416d-b579-f8c88e1151cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2evPp14fy258",
    "outputId": "89ad06a8-b56f-4a85-d12a-da1f12e201d4"
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.3.14\n",
    "!pip install langchain-openai==0.3.0\n",
    "!pip install langchain-community==0.3.14\n",
    "!pip install langgraph==0.2.64\n",
    "!pip install rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b3b6fc-dd5c-4235-b031-6e6eff7f94ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ixX--jL-4TjP",
    "outputId": "dab88099-5498-4127-c1b0-21d7127d3ff4"
   },
   "outputs": [],
   "source": [
    "!pip install markitdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c83169d-9c1b-46d0-954d-758a632071d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "H9c37cLnSrbg"
   },
   "source": [
    "## Enter Open AI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "900b28f2-0916-4a4b-b2be-10e8525f95fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cv3JzCEx_PAd",
    "outputId": "c1410dba-fb3a-4f43-bd90-f249b5979433"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e24f812-93a3-4115-91f2-e3a9ca706e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ucWRRI3QztL2"
   },
   "source": [
    "## Enter Tavily Search API Key\n",
    "\n",
    "Get a free API key from [here](https://tavily.com/#api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3bdebc-6211-42c3-8b9b-e95e834b6851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mK-1WLzOrJdb",
    "outputId": "b817f0dc-5f85-4c5c-95d1-9aca03882985"
   },
   "outputs": [],
   "source": [
    "TAVILY_API_KEY = getpass('Enter Tavily Search API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28da021e-d6f1-4255-8108-9c901fe0a274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1T0s0um5Svfa"
   },
   "source": [
    "## Setup Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41017918-cf17-468c-80b0-8387568091f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "x1YSuHNF_lbh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n",
    "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c61b7fb3-ce2e-4b6e-bc45-b8d20ec97396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "TmD0XVvXiN8u"
   },
   "source": [
    "## Utility Functions\n",
    "\n",
    "- __`process_and_truncate_texts(...)`:__ Helps in truncating the content length of content extracted from the web (documents), this is very useful especially to keep documents within the limit of LLM context windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8710a4ee-8f58-44ca-824c-fa099b2a4622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1njn9g3bfWx6",
    "outputId": "8b54c06e-083c-4be2-bdd8-7b072b7c3202"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def process_and_truncate_texts(text_docs, max_tokens=127000):\n",
    "    \"\"\"\n",
    "    Calculate token counts for a list of text documents and truncate them to fit within max_tokens.\n",
    "\n",
    "    Args:\n",
    "        text_docs (list of str): List of text documents to process.\n",
    "        max_tokens (int): Maximum token limit for truncation.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Truncated list of text documents.\n",
    "    \"\"\"\n",
    "    # Load the tokenizer (adjust the encoding name as per your use case, e.g., gpt-3.5-turbo, gpt-4, etc.)\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "    # Calculate token counts for each document\n",
    "    tokenized_docs = []\n",
    "    for text in text_docs:\n",
    "        tokens = encoding.encode(text)\n",
    "        tokenized_docs.append(tokens)\n",
    "\n",
    "    # Flatten all tokens into a single list\n",
    "    all_tokens = [token for tokens in tokenized_docs for token in tokens]\n",
    "\n",
    "    # Truncate to the maximum token limit\n",
    "    truncated_tokens = all_tokens[:max_tokens]\n",
    "\n",
    "    # Decode the truncated tokens back to text\n",
    "    truncated_texts = []\n",
    "    remaining_tokens = truncated_tokens[:]\n",
    "    for tokens in tokenized_docs:\n",
    "        if len(remaining_tokens) >= len(tokens):\n",
    "            # Add the full document if its tokens fit within the remaining budget\n",
    "            truncated_texts.append(encoding.decode(tokens))\n",
    "            remaining_tokens = remaining_tokens[len(tokens):]\n",
    "        else:\n",
    "            # Partially add tokens of the last fitting document\n",
    "            truncated_texts.append(encoding.decode(remaining_tokens))\n",
    "            break\n",
    "\n",
    "    return truncated_texts\n",
    "\n",
    "text_docs = [\n",
    "        \"This is the first document. It has some content.\",\n",
    "        \"The second document might have more content than the first.\",\n",
    "        \"This is the third document. It also has its own content.\",\n",
    "        # Add more documents as needed\n",
    "    ]\n",
    "\n",
    "truncated_texts = process_and_truncate_texts(text_docs, max_tokens=20)\n",
    "truncated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de68dcbf-9fb6-4638-bca1-7a25d67e4a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0YsRrnxNkbNu"
   },
   "source": [
    "- __`search_web_extract_info(...)`:__ Leverages Tavily to get top search results for a query and then uses Tavily extracted raw content to get the full website data and then calls `process_and_truncate_texts(...)` on the extracted documents to truncate them to a specific token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da84fe5-1bb1-4ad1-97c3-6b1d9140ec6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "MoKJ464o33xD"
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "\n",
    "tavily_search = TavilySearchAPIWrapper()\n",
    "\n",
    "\n",
    "def search_web_extract_info(query: str) -> list:\n",
    "    \"\"\"Search the web for a query and extracts useful information from the search links.\"\"\"\n",
    "    print('Calling web search tool for query:', query)\n",
    "    results = tavily_search.raw_results(query=query,\n",
    "                                        max_results=3,\n",
    "                                        search_depth='advanced',\n",
    "                                        include_answer=False,\n",
    "                                        include_raw_content=True)\n",
    "    docs = [doc['raw_content'] for doc in results['results']]\n",
    "    docs = process_and_truncate_texts(docs, max_tokens=127000)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "409504ad-d5fb-42df-b6da-e18cc1329af1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8JLlJq6DvghH",
    "outputId": "2f954079-bb11-43d3-854a-1fd2c0798ba8"
   },
   "outputs": [],
   "source": [
    "docs = search_web_extract_info('what is langgraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a711933d-02ed-47bf-a6ff-e810cf91e01a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k18GERnd4-up",
    "outputId": "c369060f-53a2-4017-e5fd-58f517f73187"
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36fef250-d310-4f0f-8391-09044177d572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uCbnqxR35AP3",
    "outputId": "3b7997ce-733d-4429-ccbd-ca31449eb41a"
   },
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da891fe7-93d9-4dec-bec0-4913c52ecca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fW72w_FSk8xZ"
   },
   "source": [
    "## Build the Web Research Sub-Agent graph - Tool-Use Agent\n",
    "\n",
    "This agent or to be more accurate sub-agent will be used in our planner to get information for each query or step in the overall plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b0ce49-e9e9-4cbf-a6ce-e76d1ec1803d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "z2YUX6YS97F8"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, RemoveMessage\n",
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tools = [search_web_extract_info]\n",
    "llm = ChatOpenAI(model=\"gpt-5\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "\n",
    "SYS_MSG = SystemMessage(content=\"Act as a helpful assistant and answer user questions\")\n",
    "def chatbot(state: State):\n",
    "    messages = trim_messages(\n",
    "            state[\"messages\"],\n",
    "            max_tokens=127000,\n",
    "            strategy=\"last\",\n",
    "            token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "            allow_partial=True,\n",
    "    )\n",
    "    return {\"messages\": [llm_with_tools.invoke([SYS_MSG] + messages)]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    "    [\"tools\", END]\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "react_agent = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10eeb7ce-0a7e-431d-9aca-85f37041f301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "reeYrrrTvtNG",
    "outputId": "dc333ed6-f2d8-43c4-a084-d60e45ac75a7"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(react_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c97389-3933-45a0-8b32-3933d161dcb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "svZ5hYm0lI53"
   },
   "source": [
    "## Test the Web Research Sub-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2299f82c-ed48-4010-9992-1950e459758e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZYQOc-De73gz"
   },
   "outputs": [],
   "source": [
    "def call_agent(agent, prompt, user_config={\"configurable\": {\"thread_id\": \"any\"}}):\n",
    "    events = agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
    "        user_config,\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "\n",
    "    for event in events:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0cba57-a040-422f-8fe5-c506dcbf621f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3qIPWLEHv-C",
    "outputId": "8bf3d112-65d5-40c6-afce-7c0e98152098"
   },
   "outputs": [],
   "source": [
    "user_prompt = \"Can you tell me about langgraph?\"\n",
    "\n",
    "call_agent(agent=react_agent,\n",
    "           prompt=user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f725cd-e105-47e7-861f-9794b44b574c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1o0b_OVBIWT",
    "outputId": "05e3a516-b3cd-4ac3-f8e1-cbd4580e80af"
   },
   "outputs": [],
   "source": [
    "user_prompt = \"Who are the key founders of Nvidia, what did they previously do and how did nvidia become such a huge company with a huge market cap?\"\n",
    "\n",
    "call_agent(agent=react_agent,\n",
    "           prompt=user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6f8cdc-ad94-4949-86fd-6f7cec2c979b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWDG5eUyLHUl",
    "outputId": "776e3454-bfc8-425d-97cb-1b747aa5f1f1"
   },
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"who is the founder of microsoft, their hometown and history of founder and the company?\n",
    "               \"\"\"\n",
    "\n",
    "call_agent(agent=react_agent,\n",
    "           prompt=user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8023e2d-4d5e-46ee-bb9c-5db672ecfd02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aVkpuUbEmwgw"
   },
   "source": [
    "## Create State Schema for our Planning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ee179a7-a853-4c0e-8e19-18332f944265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "bZKysdw9MlA_"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# state of the executed plan including overall plan, current status in terms of steps executed and output\n",
    "# response would either be the final result or errors encountered\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29b4bdb4-9a8b-44ca-bba5-d9f248bf2d4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "kzh4T_mAm-3Q"
   },
   "source": [
    "## Create AI Workflow for Plan Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a65528-7bd5-4cad-87b4-e91f61dcbb52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ncDzm7jrOC9v"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Union\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future to get to a well-researched output for user query\"\"\"\n",
    "    type: Literal[\"Plan\"]\n",
    "    steps: List[str] = Field(\n",
    "        description=\"Different steps to follow in the plan, should be in sorted order\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99302cf5-bd88-4b3f-a23c-7d8fb0512c8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3HXYRmvlPjd0"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Analyze the given query or task in detail and follow these rules to make a plan:\n",
    "                - Come up with a simple step by step plan (max 5 steps).\n",
    "                - This plan should involve individual tasks, that if executed correctly will yield the correct answer.\n",
    "                - Do not add any extra superfluous steps.\n",
    "                - The result of the final step should lead to the final answer.\n",
    "                - Make sure that each step has all the information needed - do not skip steps.\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d0f958-f9b8-4222-8ebf-40883ee507a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "SIr0ICa1QWfA"
   },
   "outputs": [],
   "source": [
    "llm_planner = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "planner = (\n",
    "            planner_prompt\n",
    "                |\n",
    "            llm_planner.with_structured_output(Plan) #function calling because we are using structured output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77868150-1a65-498f-b3c3-92b8a524f306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Vyb7R94Q52n",
    "outputId": "717a521f-ab45-4d2e-a4ab-c6921c392d70"
   },
   "outputs": [],
   "source": [
    "res = planner.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"who is the founder of microsoft\")\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e86ca9d5-d4db-437d-bf40-60b408c03699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WvDeuKoSVoYE",
    "outputId": "96c5a61f-2bc3-4e96-e942-16546b124c64"
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7b5b54-9c26-4b84-8b20-95660a0248e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "YIn7rzmRnNrv"
   },
   "source": [
    "## Create AI Workflow for Replanner (with reflection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f79b8ce1-f090-459b-9689-81067907cc93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CjruwmpXRWiL"
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response generated for the user.\"\"\"\n",
    "    type: Literal[\"Response\"]\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a33950-7c85-42ff-9625-4f35724826fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "rKvpvtJZUP1N"
   },
   "outputs": [],
   "source": [
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given objective, come up with a simple step by step plan.\n",
    "       This plan should involve individual tasks, that if executed correctly will yield the correct answer.\n",
    "       Do not add any extra superfluous steps.\n",
    "       The result of the final step should be the final answer.\n",
    "       Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "       Your main objective was this:\n",
    "       {input}\n",
    "\n",
    "       Your original plan was this:\n",
    "       {plan}\n",
    "\n",
    "       You have currently done the following steps (with their corresponding results):\n",
    "       {past_steps}\n",
    "\n",
    "       Update your plan accordingly as necessary.\n",
    "\n",
    "       If no more steps are needed then analyze all the information collected so far and generate a detailed formatted answer in Markdown.\n",
    "       When generating the final response in markdown, if there are special characters in the text, such as the dollar symbol,\n",
    "       ensure they are escaped properly for correct rendering e.g $25.5 should become \\$25.5\n",
    "\n",
    "       Otherwise, fill out the plan.\n",
    "       Only add steps to the plan that still NEED to be done.\n",
    "       Do NOT return previously done steps as part of the plan.\"\"\"\n",
    ")\n",
    "\n",
    "llm_replanner = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "replanner = (\n",
    "                replanner_prompt\n",
    "                    |\n",
    "                llm_replanner.with_structured_output(Act)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3cff55-7088-48e9-9b8a-a4fe180e1919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "kxz49jNvnnKU"
   },
   "source": [
    "## Create Node Function for Plan Generation\n",
    "\n",
    "Uses the Planner AI Workflow from above to generate a plan for the given user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96958579-5488-4c02-a7b9-59a20c0c534c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-rwEFigmb0_s"
   },
   "outputs": [],
   "source": [
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    return {\"plan\": plan.steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff086def-7350-462c-b890-386682d5923d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AeJxHzBynXVY"
   },
   "source": [
    "## Create Node Function for Plan Step Execution\n",
    "\n",
    "Executes the current step in the plan using the researcher sub-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff83276-fcf5-4cbf-8c2c-5ffa45205a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AqnQ_iCgbRLs"
   },
   "outputs": [],
   "source": [
    "react_agent = graph_builder.compile() # compile and prepare the researcher sub-agent\n",
    "\n",
    "async def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    past_steps = state['past_steps']\n",
    "    if not plan:  # Check if plan is empty\n",
    "        return {\n",
    "            \"past_steps\": [],\n",
    "            \"response\": \"No steps to execute in the plan.\"\n",
    "        }\n",
    "\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    print('=== plan str ===')\n",
    "    print(plan_str)\n",
    "    print('=== end of plan str ===')\n",
    "    task = plan[0]\n",
    "    task_formatted = f\"\"\"Given the following plan:\n",
    "                        {plan_str}\n",
    "\n",
    "                         You are tasked with executing step {1}, {task} only.\n",
    "\n",
    "                         Here are the past steps already executed, use this information also as needed.\n",
    "                         {past_steps}\n",
    "\n",
    "                         Call tools only when you need more information.\n",
    "                         If doing web search optimize the query for search before tool calling\n",
    "                         otherwise refer to all the information you have to solve the step.\n",
    "                         Do NOT call tools for analyzing existing information or compiling reports\n",
    "                         or formatting documents.\n",
    "                         \"\"\"\n",
    "\n",
    "    agent_response = await react_agent.ainvoke(\n",
    "        {\"messages\": [(\"user\", task_formatted)]}\n",
    "    )\n",
    "    return {\n",
    "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66af720-4c8f-496a-b154-f884fde29a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0vt1M94dn3Ky"
   },
   "source": [
    "## Create Node Function for Replanner\n",
    "\n",
    "Updates the current plan's next steps if needed by reflecting on the past steps executed using the Replanner AI workflow from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab497216-76b8-40f3-b3e9-09e8aa345905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Tt-rf0lrb6fL"
   },
   "outputs": [],
   "source": [
    "async def replan_step(state: PlanExecute):\n",
    "    try:\n",
    "        output = await replanner.ainvoke(state)\n",
    "        print('=== replanner output ===')\n",
    "        print(output)\n",
    "        print('=== end of replanner output ===')\n",
    "\n",
    "        # Handle dict response\n",
    "        if isinstance(output, dict) and 'Response' in output:\n",
    "            return {\"response\": output['Response']}\n",
    "\n",
    "        # Handle proper Act instance\n",
    "        if isinstance(output.action, Response):\n",
    "            return {\"response\": output.action.response}\n",
    "        else:\n",
    "            return {\"plan\": output.action.steps}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in replan_step: {e}\")\n",
    "        return {\"response\": str(output.get('Response', \"An error occurred processing the response.\"))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e697f9-8862-4da5-bc8f-b1746e5c7f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "WYqc4ZtOoFga"
   },
   "source": [
    "## Create Conditional Edge Function for Stopping the Agent\n",
    "\n",
    "Stops the agent once all steps have been executed or there is an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8b61a8-fd7a-4d66-bc4f-335f16041d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qxNdU4cKbC9f"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return END\n",
    "    else:\n",
    "        return \"react_agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a711d2-f2cf-4b8d-9dbd-bbd6bf6999f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "pJdPxQLZoXSf"
   },
   "source": [
    "## Build Planner Agent Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a1d033-0aca-450c-94a4-6f935eeac711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6hYSz0FjcQii"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "plan_graph = StateGraph(PlanExecute)\n",
    "\n",
    "# Add the plan node\n",
    "plan_graph.add_node(\"planner\", plan_step)\n",
    "# Add the execution step\n",
    "plan_graph.add_node(\"react_agent\", execute_step)\n",
    "# Add a replan node\n",
    "plan_graph.add_node(\"replanner\", replan_step)\n",
    "\n",
    "# From plan we go to agent\n",
    "plan_graph.add_edge(\"planner\", \"react_agent\")\n",
    "# From agent, we replanner\n",
    "plan_graph.add_edge(\"react_agent\", \"replanner\")\n",
    "plan_graph.add_conditional_edges(\n",
    "    \"replanner\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_end,\n",
    "    [\"react_agent\", END]\n",
    ")\n",
    "plan_graph.set_entry_point(\"planner\")\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "planner_agent = plan_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37def42b-e8bb-4ee3-a94e-0d71ba01eb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "Y3qYijHYi4tw",
    "outputId": "9991ac7e-b0fd-42a6-8131-2b95f6ebcf9e"
   },
   "outputs": [],
   "source": [
    "display(Image(planner_agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938a5306-c9c4-4683-9ebb-fd2440b7250e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-L-KT48xoZ8E"
   },
   "source": [
    "## Run and Test Planner Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ca1698-9f37-4fdc-a6b2-94fe1ea6b723",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ySdOzBMxjr_x"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from rich.markdown import Markdown as RichMarkdown\n",
    "\n",
    "\n",
    "async def call_planner_agent(agent, prompt, config={\"recursion_limit\": 50}, verbose=False):\n",
    "    events = agent.astream(\n",
    "        prompt,\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "\n",
    "    async for event in events:\n",
    "        for k, v in event.items():\n",
    "            if verbose:\n",
    "                if k != \"__end__\":\n",
    "                    display(RichMarkdown(repr(k) + ' -> ' + repr(v)))\n",
    "            if k == 'response':\n",
    "                print('='*50)\n",
    "                print('Response:')\n",
    "                display(RichMarkdown(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf52832b-d5d1-4935-bc5a-bacc03ddcdea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fYvb743jyZGD",
    "outputId": "1a62bdeb-bdbe-4719-a7a9-9d91cb71f4a8"
   },
   "outputs": [],
   "source": [
    "prompt = {\"input\": \"who is the founder of microsoft, their hometown and history of founder and the company?\"}\n",
    "await call_planner_agent(agent=planner_agent,\n",
    "                         prompt=prompt,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e19ff0-5f80-4d24-8891-8dbb7f7ef398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Gs6n--wz0tJD",
    "outputId": "9b5ca0f4-e182-42d9-bcc7-385dadbbe2f2"
   },
   "outputs": [],
   "source": [
    "prompt = {\"input\": \"Who are the key founders of Nvidia, what did they previously do and how did nvidia become such a huge company with a huge market cap?\"}\n",
    "await call_planner_agent(agent=planner_agent,\n",
    "                         prompt=prompt)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4. Project_ Build_a_Reflective_Dynamic_Planning_Agent_for_Multi_Step_Complex_Query_Analysis",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
