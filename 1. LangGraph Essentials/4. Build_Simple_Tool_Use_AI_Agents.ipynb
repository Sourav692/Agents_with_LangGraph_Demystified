{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "50fa7f8a-8764-4bb9-9968-48b681a0e4f1",
      "metadata": {
        "id": "50fa7f8a-8764-4bb9-9968-48b681a0e4f1"
      },
      "source": [
        "# Build Simple Tool-Use AI Agents in LangGraph\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will be able to:\n",
        "1. **Understand the Agent pattern** - Learn the key difference between augmented LLMs and agents\n",
        "2. **Implement feedback loops** - Connect tool outputs back to the LLM for processing\n",
        "3. **Build a complete AI Agent** - Create a system that can reason, act, and respond\n",
        "4. **Understand the ReAct pattern** - See Reasoning + Acting in practice\n",
        "\n",
        "## The Key Difference: Augmented LLM vs. AI Agent\n",
        "\n",
        "### Augmented LLM (Previous Notebook)\n",
        "```\n",
        "User → LLM → Tool → END (raw tool output)\n",
        "```\n",
        "**Problem**: Tool results go directly to the user without LLM processing!\n",
        "\n",
        "### AI Agent (This Notebook)\n",
        "```\n",
        "User → LLM → Tool → LLM → (maybe more tools) → Human-readable response\n",
        "```\n",
        "**Solution**: Feedback loop lets LLM process tool results!\n",
        "\n",
        "## What Makes an Agent \"Agentic\"?\n",
        "\n",
        "| Feature | Description |\n",
        "|---------|-------------|\n",
        "| **Autonomy** | Makes decisions without human intervention |\n",
        "| **Reasoning** | Analyzes context to decide next action |\n",
        "| **Tool Use** | Calls external tools when needed |\n",
        "| **Feedback Loop** | Processes results and can take further actions |\n",
        "| **Goal-Oriented** | Works toward completing user's objective |\n",
        "\n",
        "### Tool-based Agentic AI System\n",
        "\n",
        "- **Dynamic Decision-Making**: LLM determines whether to directly respond or invoke a tool based on the query context.\n",
        "- **Seamless Tool Integration**: External tools are integrated to handle specific tasks, such as real-time web queries or computations.\n",
        "- **Workflow Flexibility**: Conditional routing ensures efficient task delegation:\n",
        "  - Tool Required: Routes to tool execution.\n",
        "  - No Tool Required: Ends the workflow with an LLM response.\n",
        "- **Feedback Loop**: Incorporates a feedback loop to improve responses by combining LLM insights and tool outputs to further improve responses or call more tools if needed\n",
        "\n",
        "![](https://i.imgur.com/DHxiOLl.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff151ef1-fa30-482a-94da-8f49964afbc3",
      "metadata": {
        "id": "ff151ef1-fa30-482a-94da-8f49964afbc3"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SETUP: Import dependencies and LLM helper functions\n",
        "# ============================================================================\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add parent directory to path for importing helpers\n",
        "# This allows us to import utility functions from the parent directory\n",
        "sys.path.append(os.path.abspath(\"..\"))\n",
        "\n",
        "# Import LLM factory functions\n",
        "# These provide easy access to different LLM providers (Groq, OpenAI)\n",
        "from helpers.utils import get_groq_llm, get_openai_llm\n",
        "\n",
        "print(\"Setup complete! Ready to build an AI Agent.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5999f8d0-989f-4638-8ade-5c257cbadfe8",
      "metadata": {
        "id": "5999f8d0-989f-4638-8ade-5c257cbadfe8"
      },
      "source": [
        "## Step 1: Define the Agent State\n",
        "\n",
        "For an AI Agent with feedback loops, the message history becomes even more important:\n",
        "\n",
        "### Message Flow in an Agent\n",
        "```\n",
        "1. HumanMessage     - User's original query\n",
        "2. AIMessage        - LLM's response (with tool_calls)\n",
        "3. ToolMessage      - Results from tool execution\n",
        "4. AIMessage        - LLM processes tool results (may call more tools)\n",
        "5. ToolMessage      - More tool results (if needed)\n",
        "6. AIMessage        - Final human-readable response\n",
        "```\n",
        "\n",
        "The `add_messages` reducer preserves this entire chain, allowing the LLM to see:\n",
        "- What the user asked\n",
        "- What tools were called\n",
        "- What results came back\n",
        "- The full reasoning chain\n",
        "\n",
        "Let's use the `TypedDict` class from python's `typing` module as our schema, which provides type hints for the keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a90709b-ddfa-4671-8acc-c59969a29991",
      "metadata": {
        "id": "6a90709b-ddfa-4671-8acc-c59969a29991"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DEFINING AGENT STATE\n",
        "# ============================================================================\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    \"\"\"\n",
        "    State schema for the AI Agent.\n",
        "    \n",
        "    The messages list tracks the complete interaction:\n",
        "    - HumanMessage: User queries\n",
        "    - AIMessage: LLM responses (may include tool_calls)\n",
        "    - ToolMessage: Tool execution results\n",
        "    \n",
        "    The add_messages reducer ensures ALL messages are preserved,\n",
        "    which is crucial for the feedback loop where the LLM needs\n",
        "    to see what tools returned to generate a final response.\n",
        "    \n",
        "    This state structure supports the agentic pattern where:\n",
        "    1. LLM can see the full conversation history\n",
        "    2. Tool results are added to the state\n",
        "    3. LLM can process tool results in subsequent calls\n",
        "    4. Multiple tool calls can be chained together\n",
        "    \"\"\"\n",
        "    messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mMwPwbV-mNQm",
      "metadata": {
        "id": "mMwPwbV-mNQm"
      },
      "source": [
        "## Step 2: Create Tools and Augment the LLM\n",
        "\n",
        "Same as before - we create tools and bind them to the LLM.\n",
        "The difference comes in how we **structure the graph** to create a feedback loop.\n",
        "\n",
        "### Tool Creation Recap\n",
        "1. Define Python function with `@tool` decorator\n",
        "2. Write clear docstring (helps LLM decide when to use it)\n",
        "3. Add type hints for parameters\n",
        "4. Bind tools to LLM with `bind_tools()`\n",
        "\n",
        "Here we define our custom search tool and then bind it to the LLM to augment the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2lYXBn1ImzlB",
      "metadata": {
        "id": "2lYXBn1ImzlB"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CREATE TOOLS AND AUGMENT THE LLM\n",
        "# ============================================================================\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 1: Initialize the base LLM\n",
        "# We use GPT-4o for better reasoning and tool-use capabilities\n",
        "# Temperature=0 makes responses more deterministic\n",
        "# -----------------------------------------------------------------------------\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "print(f\"LLM initialized: {llm.model_name}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 2: Create the web search tool using Tavily\n",
        "# Tavily is a search API designed specifically for AI applications\n",
        "# This tool allows the agent to fetch real-time information from the internet\n",
        "# -----------------------------------------------------------------------------\n",
        "tavily_search = TavilySearchAPIWrapper()\n",
        "\n",
        "@tool\n",
        "def search_web(query: str, num_results: int = 5):\n",
        "    \"\"\"\n",
        "    Search the web for a query. Useful for general information or general news.\n",
        "    \n",
        "    This docstring is CRITICAL - it tells the LLM:\n",
        "    - WHAT the tool does (search the web)\n",
        "    - WHEN to use it (for current events, real-time info, facts beyond training data)\n",
        "    \n",
        "    Use this tool when:\n",
        "    - The user asks about current events or news\n",
        "    - The user needs real-time information\n",
        "    - The question requires facts beyond your training data\n",
        "    \n",
        "    Args:\n",
        "        query: The search query to look up\n",
        "        num_results: Number of results to return (default 5)\n",
        "        \n",
        "    Returns:\n",
        "        Search results containing titles, URLs, and content snippets\n",
        "    \"\"\"\n",
        "    print(f\"  [TOOL CALL] Searching web for: '{query}'\")\n",
        "    results = tavily_search.raw_results(\n",
        "        query=query,\n",
        "        max_results=num_results,\n",
        "        search_depth='advanced',\n",
        "        include_raw_content=True\n",
        "    )\n",
        "    return results\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 3: Create the tools list and bind to LLM\n",
        "# bind_tools() creates an \"augmented\" LLM that knows about these tools\n",
        "# This LLM can now decide when to call tools based on the user's query\n",
        "# -----------------------------------------------------------------------------\n",
        "tools = [search_web]  # List of all available tools\n",
        "\n",
        "# Bind tools to the LLM\n",
        "# This creates a new LLM that can decide to call tools when needed\n",
        "llm_with_tools = llm.bind_tools(tools=tools)\n",
        "\n",
        "print(f\"Agent equipped with {len(tools)} tool(s): {[t.name for t in tools]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wjOYMS0rn1CF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjOYMS0rn1CF",
        "outputId": "672b4674-d154-4331-aad0-100075ccd7a9"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TEST: Verify the augmented LLM can make tool calls\n",
        "# ============================================================================\n",
        "# Let's test that our LLM is properly augmented and can make tool calls\n",
        "# For queries requiring current information, the LLM should return tool_calls\n",
        "\n",
        "print(\"Testing augmented LLM with tool-calling capability:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Query: 'what is the latest news on nvidia'\")\n",
        "print(\"Expected: LLM should return tool_calls (not direct answer)\\n\")\n",
        "\n",
        "response = llm_with_tools.invoke('what is the latest news on nvidia')\n",
        "\n",
        "# Check if the LLM called any tools\n",
        "if response.tool_calls:\n",
        "    print(\"✓ LLM correctly decided to call tools!\")\n",
        "    print(f\"Tool calls: {response.tool_calls}\")\n",
        "else:\n",
        "    print(\"Note: LLM answered directly (no tool calls needed)\")\n",
        "    print(f\"Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NPFD28OWjGQM",
      "metadata": {
        "id": "NPFD28OWjGQM"
      },
      "source": [
        "## Step 3: Create the Graph with the Tool-Use Agentic System\n",
        "\n",
        "### The Critical Difference: The Feedback Loop\n",
        "\n",
        "**This is where we add the feedback loop that makes it an Agent!**\n",
        "\n",
        "In the previous notebook (Augmented LLM), tool results went directly to END:\n",
        "```\n",
        "LLM → Tool → END (raw results)\n",
        "```\n",
        "\n",
        "In this notebook (AI Agent), we add a feedback loop:\n",
        "```\n",
        "LLM → Tool → LLM (processes results) → END or more tools\n",
        "```\n",
        "\n",
        "### Key Components\n",
        "\n",
        "| Component | Purpose |\n",
        "|-----------|---------|\n",
        "| **tool_calling_llm** | Node that calls the augmented LLM |\n",
        "| **tools** (ToolNode) | Executes tool calls |\n",
        "| **tools_condition** | Routes based on whether LLM made tool calls |\n",
        "| **Feedback loop** | Routes tool results back to LLM |\n",
        "\n",
        "### The Agent Flow\n",
        "\n",
        "```\n",
        "┌─────────────────┐\n",
        "│  START          │\n",
        "└────────┬────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│tool_calling_llm │ ← LLM analyzes query\n",
        "└────────┬────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│tools_condition  │ ← Check for tool_calls\n",
        "└────────┬────────┘\n",
        "    ┌────┴────┐\n",
        "    │         │\n",
        "    ▼         ▼\n",
        "┌────────┐  ┌─────┐\n",
        "│ tools  │  │ END │ (no tools needed)\n",
        "└───┬────┘  └─────┘\n",
        "    │\n",
        "    │ [FEEDBACK LOOP] ← Tool results go back to LLM!\n",
        "    │\n",
        "    ▼\n",
        "┌─────────────────┐\n",
        "│tool_calling_llm │ ← LLM processes tool results\n",
        "└────────┬────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│tools_condition  │ ← Check again\n",
        "└────────┬────────┘\n",
        "    ┌────┴────┐\n",
        "    │         │\n",
        "    ▼         ▼\n",
        "┌────────┐  ┌─────┐\n",
        "│ tools  │  │ END │ (final response)\n",
        "└────────┘  └─────┘\n",
        "```\n",
        "\n",
        "**Note**: The feedback loop allows:\n",
        "1. LLM to process tool results\n",
        "2. Generate human-readable responses\n",
        "3. Make additional tool calls if needed\n",
        "4. Chain multiple tool calls together\n",
        "\n",
        "![](https://i.imgur.com/DHxiOLl.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IJvHs_Py3uCV",
      "metadata": {
        "id": "IJvHs_Py3uCV"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# BUILD THE TOOL-USE AI AGENT GRAPH\n",
        "# ============================================================================\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 1: Define the LLM node with tool-calling capability\n",
        "# This node calls our tool-augmented LLM\n",
        "# -----------------------------------------------------------------------------\n",
        "def tool_calling_llm(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Node that calls the tool-augmented LLM.\n",
        "    \n",
        "    The LLM will:\n",
        "    1. Analyze all messages in the state (including tool results)\n",
        "    2. Decide if tools are needed or if it can respond directly\n",
        "    3. Return either:\n",
        "       - A direct response (AIMessage with content)\n",
        "       - A tool call request (AIMessage with tool_calls)\n",
        "    \n",
        "    Note: This node is called multiple times in the feedback loop:\n",
        "    - First: Processes user query\n",
        "    - After tools: Processes tool results (this is the feedback!)\n",
        "    \"\"\"\n",
        "    current_state = state[\"messages\"]\n",
        "    response = llm_with_tools.invoke(current_state)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 2: Build the graph structure\n",
        "# -----------------------------------------------------------------------------\n",
        "builder = StateGraph(State)\n",
        "\n",
        "# Add the LLM node (makes decisions about tool use)\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "\n",
        "# Add the ToolNode (executes tools when called)\n",
        "# ToolNode is a pre-built node that:\n",
        "# - Extracts tool calls from the AIMessage\n",
        "# - Executes each tool with provided arguments\n",
        "# - Returns ToolMessage(s) with results\n",
        "builder.add_node(\"tools\", ToolNode(tools=tools))\n",
        "\n",
        "# Entry point: Start with the LLM\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 3: Add conditional routing based on tool_calls\n",
        "# tools_condition is a pre-built function that:\n",
        "# - Returns \"tools\" if the LLM response has tool_calls\n",
        "# - Returns END if no tool_calls (direct response)\n",
        "# -----------------------------------------------------------------------------\n",
        "builder.add_conditional_edges(\n",
        "    \"tool_calling_llm\",\n",
        "    tools_condition,  # Built-in routing: checks for tool_calls in response\n",
        "    [\"tools\", END]    # Possible destinations\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 4: THE KEY FEEDBACK LOOP\n",
        "# After tools execute, route results back to the LLM\n",
        "# This allows the LLM to process tool results and generate a final response\n",
        "# -----------------------------------------------------------------------------\n",
        "builder.add_edge(\"tools\", \"tool_calling_llm\")  # ← This is the feedback loop!\n",
        "\n",
        "# Also add an edge to END from tools (for termination if needed)\n",
        "# Note: This creates a potential cycle, but tools_condition will handle termination\n",
        "builder.add_edge(\"tools\", END)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 5: Compile the graph\n",
        "# -----------------------------------------------------------------------------\n",
        "agent = builder.compile()\n",
        "\n",
        "print(\"AI Agent graph compiled!\")\n",
        "print(\"Key feature: Feedback loop from tools → tool_calling_llm\")\n",
        "print(\"\\nGraph structure:\")\n",
        "display(Image(agent.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JR2L3D5Y3uH9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "JR2L3D5Y3uH9",
        "outputId": "1b606d4e-4216-4cdb-ba2a-4581df847527"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZE THE AGENT GRAPH\n",
        "# ============================================================================\n",
        "# Notice the key difference from the augmented LLM:\n",
        "# There's a feedback loop from \"tools\" back to \"tool_calling_llm\"\n",
        "# This allows the LLM to process tool results!\n",
        "\n",
        "print(\"Agent Graph Visualization:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Flow options:\")\n",
        "print(\"  1. START → tool_calling_llm → END (no tools needed)\")\n",
        "print(\"  2. START → tool_calling_llm → tools → tool_calling_llm → END\")\n",
        "print(\"     (tools called, then LLM processes results)\")\n",
        "print(\"  3. START → tool_calling_llm → tools → tool_calling_llm → tools → ...\")\n",
        "print(\"     (multiple tool calls if needed)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "agent  # Display the agent object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db16ab8d-b817-4f3a-befc-a02b579c4fca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db16ab8d-b817-4f3a-befc-a02b579c4fca",
        "outputId": "91524848-d16b-439b-953d-5396a196d083"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TEST 1: Query that DOESN'T need tools\n",
        "# ============================================================================\n",
        "# Simple factual question - LLM can answer from training data\n",
        "# Expected: Direct response, no tool calls, single pass through graph\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TEST 1: Simple query (no tools needed)\")\n",
        "print(\"Query: 'Explain AI in 2 bullets'\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "user_input = \"Explain AI in 2 bullets\"\n",
        "\n",
        "print(\"\\nStreaming agent execution:\")\n",
        "print(\"Expected flow: START → tool_calling_llm → END\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for event in agent.stream({\"messages\": user_input},\n",
        "                          stream_mode='values'):\n",
        "    event['messages'][-1].pretty_print()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Observation: Agent answered directly without calling any tools\")\n",
        "print(\"This is the same as an augmented LLM - no feedback loop needed!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h-zYQrB6383i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-zYQrB6383i",
        "outputId": "29828380-44a6-48ea-824a-87fde936bf18"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TEST 2: Query that NEEDS tools (current information)\n",
        "# ============================================================================\n",
        "# This question requires current/real-time information\n",
        "# Expected flow: \n",
        "#   1. LLM calls search_web tool\n",
        "#   2. Tool returns results\n",
        "#   3. LLM processes tool results (feedback loop!)\n",
        "#   4. LLM generates human-readable final response\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TEST 2: Query requiring current information (tools needed)\")\n",
        "print(\"Query: 'What is the latest news on OpenAI product releases'\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "user_input = \"What is the latest news on OpenAI product releases\"\n",
        "\n",
        "print(\"\\nStreaming agent execution:\")\n",
        "print(\"Expected flow: START → tool_calling_llm → tools → tool_calling_llm → END\")\n",
        "print(\"Notice: Two LLM calls! (initial decision + processing tool results)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for event in agent.stream({\"messages\": user_input},\n",
        "                          stream_mode='values'):\n",
        "    event['messages'][-1].pretty_print()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Observation: Agent used the feedback loop!\")\n",
        "print(\"1. LLM decided to call search_web tool\")\n",
        "print(\"2. Tool executed and returned results\")\n",
        "print(\"3. LLM processed tool results and generated human-readable response\")\n",
        "print(\"This is the key difference from an augmented LLM - the agent processes tool results!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ICTlpMAqnL8L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICTlpMAqnL8L",
        "outputId": "07dbb9e8-5f69-4f5e-dc51-ee5e19c91ac3"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXPLORE: Examine the final message\n",
        "# ============================================================================\n",
        "# The final message should be an AIMessage with a human-readable response\n",
        "# that incorporates the tool results\n",
        "\n",
        "print(\"Final message in the conversation:\")\n",
        "print(\"-\" * 70)\n",
        "event['messages'][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T4h42-u94Jmb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "T4h42-u94Jmb",
        "outputId": "6e2b715e-849d-47fd-c639-eafcdcae93b4"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DISPLAY: View the final human-readable response\n",
        "# ============================================================================\n",
        "# This shows the agent's final response after processing tool results\n",
        "# Compare this to the raw tool output - the agent has synthesized it into\n",
        "# a coherent, human-readable answer\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "print(\"Final Agent Response (formatted as Markdown):\")\n",
        "print(\"=\" * 70)\n",
        "display(Markdown(event['messages'][-1].content))\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Note: This is a synthesized response based on tool results,\")\n",
        "print(\"not just the raw tool output!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb2b05df",
      "metadata": {},
      "source": [
        "## Summary: Key Takeaways\n",
        "\n",
        "### What We Built\n",
        "A complete **AI Agent** that can:\n",
        "- Decide when to use external tools\n",
        "- Execute web searches for current information\n",
        "- **Process tool results** (the key difference!)\n",
        "- Generate human-readable responses from tool outputs\n",
        "- Chain multiple tool calls if needed\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **Feedback Loop** | Routes tool results back to LLM for processing |\n",
        "| **Agent vs Augmented LLM** | Agents process tool results; augmented LLMs just execute tools |\n",
        "| **ToolNode** | Pre-built node that executes tool calls |\n",
        "| **tools_condition** | Pre-built routing based on tool_calls |\n",
        "\n",
        "### The Agent Pattern\n",
        "\n",
        "```\n",
        "1. User Query → LLM\n",
        "2. LLM decides: Answer directly OR call tools\n",
        "3a. Direct answer → Return response → END\n",
        "3b. Call tools → ToolNode executes → Results back to LLM (feedback loop!)\n",
        "4. LLM processes tool results → Generate human-readable response → END\n",
        "```\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "**Without feedback loop (Augmented LLM):**\n",
        "- Tool results go directly to user\n",
        "- Raw, unprocessed output\n",
        "- No synthesis or reasoning\n",
        "\n",
        "**With feedback loop (AI Agent):**\n",
        "- LLM processes tool results\n",
        "- Generates coherent, synthesized responses\n",
        "- Can make additional tool calls based on results\n",
        "- True agentic behavior!\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "You've now built a complete AI Agent! Next steps could include:\n",
        "- Adding multiple tools (calculator, database queries, etc.)\n",
        "- Implementing memory/persistence for conversations\n",
        "- Adding error handling and retry logic\n",
        "- Building multi-agent systems\n",
        "- Adding human-in-the-loop confirmation for critical actions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
