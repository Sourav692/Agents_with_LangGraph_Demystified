{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50fa7f8a-8764-4bb9-9968-48b681a0e4f1"
      },
      "source": [
        "# Build a LLM-powered Chatbot in LangGraph\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will be able to:\n",
        "1. **Integrate LLMs into LangGraph** - Connect language models to your graph nodes\n",
        "2. **Build a functional chatbot** - Create an end-to-end conversational AI\n",
        "3. **Understand invoke vs. stream** - Learn different ways to run your graph\n",
        "4. **Handle message types** - Work with HumanMessage and AIMessage objects\n",
        "\n",
        "## What We're Building\n",
        "A simple but complete LLM-powered chatbot that:\n",
        "- Receives user messages\n",
        "- Sends them to an LLM (Groq/OpenAI)\n",
        "- Returns the AI response\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                    CHATBOT ARCHITECTURE                         │\n",
        "├─────────────────────────────────────────────────────────────────┤\n",
        "│                                                                  │\n",
        "│    User Input ──► [chatbot node] ──► LLM ──► AI Response        │\n",
        "│                        │                        │                │\n",
        "│                        └────────────────────────┘                │\n",
        "│                              (state update)                      │\n",
        "│                                                                  │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "LangGraph is not just a framework to create static graphs. We already know that it can be used for building stateful, agentic applications using LLMs.\n",
        "\n",
        "We'll now create a simple LLM-powered chatbot using LangGraph. This chatbot will respond directly to user messages.\n",
        "\n",
        "![](https://i.imgur.com/heeggTe.png)"
      ],
      "id": "50fa7f8a-8764-4bb9-9968-48b681a0e4f1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================================\n",
        "# SETUP: Import LLM Helper Functions\n",
        "# ============================================================================\n",
        "# We use helper functions to create LLM instances with proper configuration\n",
        "# These functions handle API key loading and model configuration\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add parent directory to path for importing helpers\n",
        "sys.path.append(os.path.abspath(\"..\"))\n",
        "\n",
        "# Import our LLM factory functions\n",
        "# - get_groq_llm(): Creates a Groq-hosted LLM (fast inference)\n",
        "# - get_openai_llm(): Creates an OpenAI GPT model\n",
        "from helpers.utils import get_groq_llm, get_openai_llm\n",
        "\n",
        "print(\"LLM helpers imported successfully!\")"
      ],
      "execution_count": 1,
      "outputs": [],
      "id": "87eeb205"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5999f8d0-989f-4638-8ade-5c257cbadfe8"
      },
      "source": [
        "## Step 1: Define the State\n",
        "\n",
        "For a chatbot, we need to track the **conversation history** - all messages exchanged between user and AI.\n",
        "\n",
        "### Why Conversation History Matters\n",
        "- LLMs are **stateless** - they don't remember previous messages\n",
        "- We must send the **full conversation** with each request\n",
        "- The `add_messages` reducer automatically **appends** new messages\n",
        "\n",
        "### Message Types in LangChain\n",
        "| Type | Description | Example |\n",
        "|------|-------------|---------|\n",
        "| `HumanMessage` | User's input | \"What is AI?\" |\n",
        "| `AIMessage` | LLM's response | \"AI is...\" |\n",
        "| `SystemMessage` | System instructions | \"You are a helpful assistant\" |\n",
        "| `ToolMessage` | Tool call results | `{\"result\": \"...\"}` |\n",
        "\n",
        "The State schema serves as the input schema for all Nodes and Edges in the graph.\n",
        "\n",
        "Let's use the `TypedDict` class from python's `typing` module as our schema, which provides type hints for the keys."
      ],
      "id": "5999f8d0-989f-4638-8ade-5c257cbadfe8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a90709b-ddfa-4671-8acc-c59969a29991"
      },
      "source": [
        "# ============================================================================\n",
        "# DEFINING CHATBOT STATE\n",
        "# ============================================================================\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "class State(TypedDict):\n",
        "    \"\"\"\n",
        "    Chatbot State Schema.\n",
        "    \n",
        "    Attributes:\n",
        "        messages: List of conversation messages (HumanMessage, AIMessage)\n",
        "                 Uses add_messages reducer to append (not overwrite) messages\n",
        "    \n",
        "    The add_messages reducer is essential for chatbots because:\n",
        "    1. It preserves the full conversation history\n",
        "    2. Each new message is appended to the list\n",
        "    3. The LLM can see all previous context\n",
        "    \"\"\"\n",
        "    messages: Annotated[list, add_messages]  # Automatically appends new messages"
      ],
      "execution_count": 2,
      "outputs": [],
      "id": "6a90709b-ddfa-4671-8acc-c59969a29991"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "888509e1-cbde-4c03-99a0-2560dd2e262d"
      },
      "source": [
        "## Step 2: Create the Chatbot Node and Graph\n",
        "\n",
        "Now we'll create:\n",
        "1. **LLM instance** - The language model that generates responses\n",
        "2. **Chatbot node** - A function that calls the LLM with the current messages\n",
        "3. **Graph** - Simple structure: START → chatbot → END\n",
        "\n",
        "### How the Chatbot Node Works\n",
        "```python\n",
        "def chatbot(state):\n",
        "    # 1. Get all messages from state\n",
        "    messages = state[\"messages\"]\n",
        "    \n",
        "    # 2. Send to LLM (includes full conversation history)\n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    # 3. Return response (will be appended to messages)\n",
        "    return {\"messages\": [response]}\n",
        "```"
      ],
      "id": "888509e1-cbde-4c03-99a0-2560dd2e262d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJvHs_Py3uCV"
      },
      "source": [
        "# ============================================================================\n",
        "# CREATE THE LLM AND CHATBOT GRAPH\n",
        "# ============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 1: Initialize the LLM\n",
        "# We use Groq for fast inference, but you can swap to OpenAI\n",
        "# -----------------------------------------------------------------------------\n",
        "llm = get_groq_llm()  # Fast, open-source models hosted by Groq\n",
        "# Alternative: llm = get_openai_llm()  # OpenAI's GPT models\n",
        "\n",
        "print(f\"LLM initialized: {llm.model_name if hasattr(llm, 'model_name') else 'Groq LLM'}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 2: Define the chatbot node function\n",
        "# This is where the magic happens - the LLM is called here\n",
        "# -----------------------------------------------------------------------------\n",
        "def chatbot(state: State):\n",
        "    \"\"\"\n",
        "    The core chatbot node that calls the LLM.\n",
        "    \n",
        "    Process:\n",
        "    1. Receives the full conversation history from state\n",
        "    2. Sends all messages to the LLM for context\n",
        "    3. Returns the AI response (which gets appended to messages)\n",
        "    \n",
        "    Args:\n",
        "        state: Contains 'messages' - the full conversation history\n",
        "        \n",
        "    Returns:\n",
        "        dict with 'messages' key containing the LLM's response\n",
        "        The add_messages reducer will append this to existing messages\n",
        "    \"\"\"\n",
        "    # Get current conversation history\n",
        "    current_messages = state[\"messages\"]\n",
        "    \n",
        "    # Call the LLM with the full conversation context\n",
        "    # The LLM sees all previous messages for context\n",
        "    ai_response = llm.invoke(current_messages)\n",
        "    \n",
        "    # Return as a list - add_messages reducer will append to state\n",
        "    return {\"messages\": [ai_response]}\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 3: Build the graph\n",
        "# Simple structure: START → chatbot → END\n",
        "# -----------------------------------------------------------------------------\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Add the chatbot node\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "# Define the flow\n",
        "graph_builder.add_edge(START, \"chatbot\")  # Entry: go to chatbot\n",
        "graph_builder.add_edge(\"chatbot\", END)     # Exit: end after chatbot responds\n",
        "\n",
        "# Compile the graph\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "print(\"Chatbot graph compiled successfully!\")"
      ],
      "execution_count": 3,
      "outputs": [],
      "id": "IJvHs_Py3uCV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "JR2L3D5Y3uH9",
        "outputId": "329fe3a3-da61-4ff8-b03a-43e85e04179a"
      },
      "source": [
        "# ============================================================================\n",
        "# VISUALIZE THE CHATBOT GRAPH\n",
        "# ============================================================================\n",
        "# The graph is simple: just one node that calls the LLM\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "print(\"Chatbot Graph Structure:\")\n",
        "print(\"-\" * 40)\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "print(\"-\" * 40)\n",
        "print(\"Flow: START → chatbot (LLM call) → END\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x117d39940>"
            ]
          }
        }
      ],
      "id": "JR2L3D5Y3uH9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp5IhgSF3uKA",
        "outputId": "d91ec300-99e0-4ae9-bfd0-6e0610d4bae9"
      },
      "source": [
        "# ============================================================================\n",
        "# TEST THE CHATBOT: Basic Invocation\n",
        "# ============================================================================\n",
        "# graph.invoke() runs the graph synchronously and returns the final state\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Testing Chatbot with: 'Explain AI in 2 bullet points'\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Invoke the graph with a user message\n",
        "response = graph.invoke({\"messages\": \"Explain AI in 2 bullet points\"})\n",
        "\n",
        "# Examine the response structure\n",
        "print(f\"\\nResponse contains {len(response['messages'])} messages:\")\n",
        "for i, msg in enumerate(response['messages']):\n",
        "    msg_type = type(msg).__name__\n",
        "    print(f\"  [{i}] {msg_type}: {msg.content[:50]}...\" if len(msg.content) > 50 else f\"  [{i}] {msg_type}: {msg.content}\")\n",
        "\n",
        "response"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Explain AI in 2 bullet points', additional_kwargs={}, response_metadata={}, id='f227f82e-10ec-40e3-90aa-c35641e43e30'),\n",
              "  AIMessage(content='- **AI is the development of computer systems that can perform tasks requiring human‑like intelligence**, such as learning from data, recognizing patterns, making decisions, and understanding natural language.  \\n- **It combines techniques like machine learning, deep learning, and symbolic reasoning** to enable machines to adapt, improve, and solve problems across domains—from image recognition and speech translation to autonomous vehicles and personalized recommendations.', additional_kwargs={'reasoning_content': 'The user wants a concise explanation of AI in 2 bullet points. Provide two bullet points summarizing AI. Should be clear.'}, response_metadata={'token_usage': {'completion_tokens': 115, 'prompt_tokens': 78, 'total_tokens': 193, 'completion_time': 0.244015292, 'completion_tokens_details': {'reasoning_tokens': 27}, 'prompt_time': 0.002921689, 'prompt_tokens_details': None, 'queue_time': 0.054506591, 'total_time': 0.246936981}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8a618bed98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bdba9-4c69-7122-b22f-b9a5f4c4644a-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 78, 'output_tokens': 115, 'total_tokens': 193, 'output_token_details': {'reasoning': 27}})]}"
            ]
          }
        }
      ],
      "id": "Pp5IhgSF3uKA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsaTAPtdKAbo",
        "outputId": "fc2e2bcb-2fb0-4fc4-97e5-f34ade7899ce"
      },
      "source": [
        "# ============================================================================\n",
        "# EXTRACT THE AI RESPONSE\n",
        "# ============================================================================\n",
        "# The last message in the list is always the most recent (AI's response)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"AI Response:\")\n",
        "print(\"=\" * 60)\n",
        "print(response['messages'][-1].content)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- **AI is the development of computer systems that can perform tasks requiring human‑like intelligence**, such as learning from data, recognizing patterns, making decisions, and understanding natural language.  \n",
            "- **It combines techniques like machine learning, deep learning, and symbolic reasoning** to enable machines to adapt, improve, and solve problems across domains—from image recognition and speech translation to autonomous vehicles and personalized recommendations.\n"
          ]
        }
      ],
      "id": "gsaTAPtdKAbo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmcpBvaFf0TW"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 3: Invoking vs. Streaming in LangGraph\n",
        "\n",
        "LangGraph provides two main ways to run your graph:\n",
        "\n",
        "### `graph.invoke()` - Synchronous Execution\n",
        "- Runs the **entire graph** and returns the **final state**\n",
        "- Waits for completion before returning\n",
        "- Best for: Simple use cases, batch processing\n",
        "\n",
        "### `graph.stream()` - Streaming Execution  \n",
        "- Returns an **iterator** that yields **intermediate states**\n",
        "- Shows progress as each node completes\n",
        "- Best for: Real-time UIs, long-running workflows\n",
        "\n",
        "### Stream Modes\n",
        "\n",
        "| Mode | What it yields | Use case |\n",
        "|------|----------------|----------|\n",
        "| `'values'` | Full state after each node | See complete state evolution |\n",
        "| `'updates'` | Only the changes from each node | Efficient for large states |\n",
        "\n",
        "```\n",
        "invoke():  [wait...wait...wait...] → Final Result\n",
        "\n",
        "stream():  Node1 → Node2 → Node3 → ... (real-time updates)\n",
        "```"
      ],
      "id": "LmcpBvaFf0TW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3Tc4fJhoL9R",
        "outputId": "924a31ec-5070-4958-88a1-82ed19aafefb"
      },
      "source": [
        "# ============================================================================\n",
        "# INVOKE: Get final result in one call\n",
        "# ============================================================================\n",
        "# invoke() waits for the entire graph to complete before returning\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Using graph.invoke() - Synchronous Execution\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "response = graph.invoke({\"messages\": \"Explain AI in 1 line to a child\"})\n",
        "\n",
        "print(\"Response received after graph completion:\")\n",
        "print(\"-\" * 40)\n",
        "print(response['messages'][-1].content)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AI is like a smart robot brain that learns to help you do things, just like a friendly helper that gets better the more it plays and learns.\n"
          ]
        }
      ],
      "id": "A3Tc4fJhoL9R"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEji1u3MX9OR",
        "outputId": "1799d7c9-df75-4e7e-f04b-025237cf7cd6"
      },
      "source": [
        "# ============================================================================\n",
        "# IMPORTANT: Each invoke() is a NEW conversation!\n",
        "# ============================================================================\n",
        "# Note: Each call to invoke() starts with a FRESH state\n",
        "# The LLM doesn't remember previous invoke() calls\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Testing Memory: 'What did we discuss so far?'\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "response = graph.invoke({\"messages\": \"What did we discuss so far?\"})\n",
        "print(response['messages'][-1].content)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"NOTE: The LLM doesn't remember previous conversations!\")\n",
        "print(\"Each invoke() call starts fresh. For true multi-turn chat,\")\n",
        "print(\"you need to pass the full conversation history each time,\")\n",
        "print(\"or use LangGraph's checkpointing/persistence features.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We haven’t covered any specific topics yet—this is the first question you’ve asked in our conversation. Is there something particular you’d like to discuss or ask about?\n"
          ]
        }
      ],
      "id": "gEji1u3MX9OR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db16ab8d-b817-4f3a-befc-a02b579c4fca",
        "outputId": "5f4ca625-9344-4c5b-e9c0-05e5f77ba94d"
      },
      "source": [
        "# ============================================================================\n",
        "# STREAM MODE: 'values' - See full state after each node\n",
        "# ============================================================================\n",
        "# stream_mode='values' yields the COMPLETE state after each node executes\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Using graph.stream() with stream_mode='values'\")\n",
        "print(\"=\" * 60)\n",
        "print(\"This shows the full messages list after each step:\\n\")\n",
        "\n",
        "step = 0\n",
        "for event in graph.stream({\"messages\": \"Explain AI in 1 line to a child\"},\n",
        "                          stream_mode='values'):\n",
        "    step += 1\n",
        "    print(f\"Step {step}: {len(event['messages'])} message(s)\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Show each message in the current state\n",
        "    for i, msg in enumerate(event['messages']):\n",
        "        msg_type = type(msg).__name__\n",
        "        content = msg.content if hasattr(msg, 'content') else str(msg)\n",
        "        print(f\"  [{i}] {msg_type}: {content[:80]}{'...' if len(content) > 80 else ''}\")\n",
        "    print()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[HumanMessage(content='Explain AI in 1 line to a child', additional_kwargs={}, response_metadata={}, id='47d10bc1-b98d-420a-99e4-f02426bdd2fd')]\n",
            "[HumanMessage(content='Explain AI in 1 line to a child', additional_kwargs={}, response_metadata={}, id='47d10bc1-b98d-420a-99e4-f02426bdd2fd'), AIMessage(content='AI is like a smart robot brain that learns to help you do things, just like a friendly helper that gets better the more it plays and learns.', additional_kwargs={'reasoning_content': 'We need to respond with a single line explanation suitable for a child. Should be simple.'}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 80, 'total_tokens': 138, 'completion_time': 0.122589043, 'completion_tokens_details': {'reasoning_tokens': 19}, 'prompt_time': 0.003006535, 'prompt_tokens_details': None, 'queue_time': 0.055208476, 'total_time': 0.125595578}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_d29d1d1418', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bdba9-5e1a-7b52-8129-8cc9c5b2f7d1-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 80, 'output_tokens': 58, 'total_tokens': 138, 'output_token_details': {'reasoning': 19}})]\n"
          ]
        }
      ],
      "id": "db16ab8d-b817-4f3a-befc-a02b579c4fca"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-zYQrB6383i",
        "outputId": "b7775bba-7686-43de-d70a-007845f6f32f"
      },
      "source": [
        "# ============================================================================\n",
        "# STREAM MODE: 'updates' - See only changes from each node\n",
        "# ============================================================================\n",
        "# stream_mode='updates' yields ONLY what changed (more efficient for large states)\n",
        "# The key is the node name that produced the update\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Using graph.stream() with stream_mode='updates'\")\n",
        "print(\"=\" * 60)\n",
        "print(\"This shows only the updates from each node:\\n\")\n",
        "\n",
        "for event in graph.stream({\"messages\": \"Explain AI in 1 line to a child\"},\n",
        "                          stream_mode='updates'):\n",
        "    # event is a dict with node_name as key\n",
        "    for node_name, update in event.items():\n",
        "        print(f\"Node '{node_name}' produced update:\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        # Show what this node added\n",
        "        if 'messages' in update:\n",
        "            for msg in update['messages']:\n",
        "                msg_type = type(msg).__name__\n",
        "                content = msg.content if hasattr(msg, 'content') else str(msg)\n",
        "                print(f\"  {msg_type}: {content}\")\n",
        "        print()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'chatbot': {'messages': [AIMessage(content='AI is like a smart robot brain that learns to help you do things, just like a friendly helper that gets better the more it plays and learns.', additional_kwargs={'reasoning_content': 'We need to respond with a single line explanation suitable for a child. Should be simple.'}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 80, 'total_tokens': 138, 'completion_time': 0.129555139, 'completion_tokens_details': {'reasoning_tokens': 19}, 'prompt_time': 0.003004147, 'prompt_tokens_details': None, 'queue_time': 0.054257953, 'total_time': 0.132559286}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8a618bed98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bdba9-6634-7a93-886c-c9b4cbad334f-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 80, 'output_tokens': 58, 'total_tokens': 138, 'output_token_details': {'reasoning': 19}})]}}\n"
          ]
        }
      ],
      "id": "h-zYQrB6383i"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: Key Takeaways\n",
        "\n",
        "### What We Built\n",
        "A functional LLM-powered chatbot using LangGraph with:\n",
        "- State management for conversation history\n",
        "- LLM integration via helper functions\n",
        "- Both synchronous and streaming execution\n",
        "\n",
        "### Key Concepts Learned\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **State with add_messages** | Preserves conversation history |\n",
        "| **LLM Node** | Calls the language model with full context |\n",
        "| **invoke()** | Synchronous execution, returns final state |\n",
        "| **stream()** | Iterative execution, yields intermediate states |\n",
        "\n",
        "### Stream Modes Comparison\n",
        "\n",
        "```\n",
        "stream_mode='values':   Full state after each step\n",
        "                       Good for: Debugging, understanding flow\n",
        "\n",
        "stream_mode='updates':  Only changes from each node  \n",
        "                       Good for: Efficiency, real-time UIs\n",
        "```\n",
        "\n",
        "### Important Limitations (Current Version)\n",
        "- **No built-in memory** - Each invoke() starts fresh\n",
        "- **No persistence** - Conversation lost after session ends\n",
        "- **Solution**: Use LangGraph's checkpointing features (covered in advanced topics)\n",
        "\n",
        "### Next Steps\n",
        "In the next notebook, we'll learn how to **augment the LLM with tools** - giving it the ability to search the web, perform calculations, and more!"
      ],
      "id": "07f0138a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4h42-u94Jmb"
      },
      "source": [],
      "execution_count": null,
      "outputs": [],
      "id": "T4h42-u94Jmb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "execution_count": null,
      "outputs": [],
      "id": "6b782c65"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}